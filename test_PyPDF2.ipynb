{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter,HTMLConverter,XMLConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from cStringIO import StringIO\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_correct_page_number(filename):\n",
    "    pdf = PyPDF2.PdfFileReader(open(filename, \"rb\")) \n",
    "    page_number = 6 \n",
    "    page_index = page_number - 1\n",
    "  \n",
    "    find_page = False\n",
    "    while page_index<10 and not find_page: \n",
    "        page = pdf.getPage(page_index).extractText()\n",
    "        if 'Job Summary' in page:\n",
    "            print('correct page')\n",
    "            find_page = True\n",
    "        else:\n",
    "            print('no')\n",
    "            page_index += 1\n",
    "    page_number = page_index +1 \n",
    "    return page,page_number\n",
    "\n",
    "def convert_pdf_to_txt(path,page_number):\n",
    "    \n",
    "    page_index= page_number-1\n",
    "    rsrcmgr = PDFResourceManager() # task with interpreter and device\n",
    "    retstr = StringIO() # make it faster\n",
    "    codec = 'utf-8' # code format\n",
    "    laparams = LAParams() # help with document extraction\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams) # export into retstr\n",
    "    fp = file(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    pages= PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True)\n",
    "    for index,page in enumerate(pages):\n",
    "        if index == page_index:\n",
    "            interpreter.process_page(page)\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    string = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return string\n",
    "\n",
    "def convert_pdf_to_html(path,page_number):\n",
    "    #outfile = file(outpath,'w')\n",
    "    page_index= page_number-1\n",
    "    rsrcmgr = PDFResourceManager() # task with interpreter and device\n",
    "    retstr = StringIO() # make it faster\n",
    "    codec = 'utf-8' # code format\n",
    "    laparams = LAParams() # help with document extraction\n",
    "    device = HTMLConverter(rsrcmgr, retstr, codec=codec, laparams=laparams) # export into retstr\n",
    "    fp = file(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    pages= PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True)\n",
    "    for index,page in enumerate(pages):\n",
    "        if index == page_index:\n",
    "            interpreter.process_page(page)\n",
    "    \n",
    "    fp.close()\n",
    "    device.close()\n",
    "    string=retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return string\n",
    "\n",
    "def my_patterns():\n",
    "    patterns = dict()\n",
    "    patterns['stage']='.*Stage (\\d+) \\n'\n",
    "    patterns['text_name']='.*\\n([A-Z].*|\\d{1,3} [A-Z].*|\\d{1,3}/.*)'\n",
    "    patterns['seperate_value_unit'] = '(\\d+|\\d*.\\d+|\\d*,\\d*)(?:[a-z\\s\\*])'\n",
    "    return patterns\n",
    "\n",
    "def find_columns_names(text_string,patterns):\n",
    "    text_string_0 = text_string.index('Start Time') # find the first Start Time string\n",
    "    text_string_1 = text_string.index('Disclaimer')\n",
    "    text_split1 = text_string[text_string_0-1:text_string_1] # in order to contains \\n for the Start Time\n",
    "    columns_name_text = re.findall(patterns['text_name'],text_split1)\n",
    "    stage_number = re.findall(patterns['stage'],text_split1)\n",
    "    \n",
    "    # remove the white space \n",
    "    columns_name_text = [e.rstrip() for e in columns_name_text]\n",
    "    \n",
    "    #print(columns_name_text)\n",
    "    return columns_name_text,stage_number\n",
    "\n",
    "def remove_enter_from_string(string):\n",
    "    return string.replace('\\n','')\n",
    "\n",
    "def remove_last_white_space(string,index):\n",
    "    match = re.search('\\s',string)\n",
    "    index_space = [i for i in range(len(string)) if string.startswith(' ', i)]\n",
    "    popout = index_space.pop(index)\n",
    "    string_new = string[0:popout] + string[popout+1:]\n",
    "    return string_new\n",
    "\n",
    "\n",
    "def find_data_names_from_string(strings,columns):\n",
    "    index_start = list() # record the start of value\n",
    "    index_end = list() # recrod the end of the value\n",
    "    for index,col in enumerate(columns):\n",
    "        #print('name',col)\n",
    "        if col in strings:\n",
    "            the_index_start = strings.index(col)\n",
    "            the_index_end = the_index_start + len(col)\n",
    "        elif remove_last_white_space(col,0) in strings:        \n",
    "            print('not in, try remove first')\n",
    "            col_new = remove_last_white_space(col,0)\n",
    "            print(col_new)\n",
    "            the_index_start = strings.index(col_new)\n",
    "            the_index_end = the_index_start + len(col_new)\n",
    "        elif remove_last_white_space(col,-1) in strings:\n",
    "            print('not in, try remove last one')\n",
    "            col_new = remove_last_white_space(col,-1)\n",
    "            print(col_new)\n",
    "            the_index_start = strings.index(col_new)\n",
    "            the_index_end = the_index_start + len(col_new)\n",
    "        index_end.append(the_index_start)\n",
    "        if index == len(columns)-1: # end of column, the first element should be deleted\n",
    "            index_end.append(len(strings))\n",
    "            index_end.pop(0)\n",
    "        #elif index > 0: \n",
    "        index_start.append(the_index_end)\n",
    "        #index_start.append(match.end()+1)\n",
    "    #for index,e in enumerate(index_end):\n",
    "    index_range_list = [(index_start[index],e) for index,e in enumerate(index_end)]\n",
    "    #print(my_list)\n",
    "    #name = \n",
    "    data = [strings[e1:e2] for e1,e2 in index_range_list]\n",
    "    return data\n",
    "\n",
    "def split_value_unit_from_string(data,pattern,columns):\n",
    "    #pattern = \n",
    "    #match=re.search(pattern_unit,'.96psi/ft')\n",
    "    data_value = ['']*len(data) # record of values \n",
    "    data_unit = ['']*len(data) # record of units\n",
    "    for i,elem in enumerate(data):\n",
    "        if 'Start Time' in columns[i] or 'End Time' in columns[i]:\n",
    "            data_value[i] = elem.rstrip( )\n",
    "            data_unit[i] = 'sec'\n",
    "        else:\n",
    "            #print('col',elem)\n",
    "            match=re.search(pattern,elem)\n",
    "            data_value[i] = match.group(0).rstrip( )\n",
    "            if ',' in data_value[i]:\n",
    "                data_value[i]=data_value[i].replace(',','')\n",
    "                \n",
    "            data_unit[i] = elem[match.end():].rstrip( ) # remove the space at the end\n",
    "    return data_value,data_unit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "correct page\n",
      "                            1name           data_value    unit\n",
      "0                      Start Time   06-Mar-12 09:29:41     sec\n",
      "1                        End Time   06-Mar-12 13:31:29     sec\n",
      "2                       Pump Time               190.52     min\n",
      "3           Max Treating Pressure                11429     psi\n",
      "4           Avg Treating Pressure                10139     psi\n",
      "5                 Max Slurry Rate                 73.8     bpm\n",
      "6                 Avg Slurry Rate                 62.1     bpm\n",
      "7                   Slurry Volume               424446     gal\n",
      "8                         Avg HHP                15427      hp\n",
      "9      Max Proppant Concentration                 3.76  lb/gal\n",
      "10  BH Max Proppant Concentration                 3.76  lb/gal\n",
      "11           Proppant Mass Pumped              3416.91  100*lb\n",
      "12                       100 Mesh                40300      lb\n",
      "13             Premium White 4/70               274480      lb\n",
      "14                  Premium 30/50                47900      lb\n",
      "15                Load to Recover               470286     gal\n",
      "16               Water Frac G (5)               267911     gal\n",
      "17                   Hybor G (27)               150134     gal\n",
      "18                           NaCl                15182     gal\n",
      "19        Slurry Water Frac G (5)               270054     gal\n",
      "20            Slurry Hybor G (27)               154892     gal\n",
      "21                           ISIP                 8131     psi\n",
      "22                  Frac Gradient                 1.06  psi/ft\n"
     ]
    }
   ],
   "source": [
    "def main(filename):\n",
    "    # initiation of patterns\n",
    "    patterns = my_patterns()  \n",
    "    # get page from PyPDF2 as it follows the order\n",
    "    page,page_number=find_correct_page_number(filename)\n",
    "    # get text from pdfminer, as columns names are in the order\n",
    "    text_string = convert_pdf_to_txt(filename,page_number)\n",
    "    # get columns from text_string\n",
    "    columns, stage = find_columns_names(text_string,patterns)\n",
    "    # get value from page using columns from text_string\n",
    "    # make it easier to extract\n",
    "    page = page.split('Disclaimer')[0] \n",
    "    # remove \\n from the page \n",
    "    page_no_enter = remove_enter_from_string(page)\n",
    "    # find the data contains value and unit\n",
    "    data = find_data_names_from_string(page_no_enter,columns)\n",
    "    # split data into value and unit\n",
    "    data_value,data_unit = split_value_unit_from_string(data,patterns['seperate_value_unit'],columns)\n",
    "    # create the dataframe\n",
    "    DataFrame= pd.DataFrame({'1name':columns,'data_value':np.array(data_value),'unit':pd.Series(data_unit)})\n",
    "    \n",
    "    return DataFrame\n",
    "filename = 'TCC Encana Oil  Gas - Horseshoe Hill 10H-1 Stg 15.pdf'\n",
    "DataFrame = main(filename)\n",
    "print(DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "correct page\n",
      "                            1name           data_value    unit\n",
      "0                      Start Time   02-Mar-12 21:11:33     sec\n",
      "1                        End Time   03-Mar-12 00:03:18     sec\n",
      "2                       Pump Time                  172     min\n",
      "3           Max Treating Pressure                 9983     psi\n",
      "4           Avg Treating Pressure                 9124     psi\n",
      "5                 Max Slurry Rate                 61.0     bpm\n",
      "6                 Avg Slurry Rate                 60.1     bpm\n",
      "7                   Slurry Volume               378137     gal\n",
      "8                         Avg HHP                13440      hp\n",
      "9      Max Proppant Concentration                 3.96  lb/gal\n",
      "10  BH Max Proppant Concentration                 3.95  lb/gal\n",
      "11           Proppant Mass Pumped               360000      lb\n",
      "12                       100 Mesh                40000      lb\n",
      "13            40/70 Premium White               280000      lb\n",
      "14                30/50 Interprop                40000      lb\n",
      "15                Load to Recover               361872     gal\n",
      "16               Water Frac G (5)               203584     gal\n",
      "17                   Hybor G (27)               137631     gal\n",
      "18        Slurry Water Frac G (5)               219243     gal\n",
      "19            Slurry Hybor G (27)               158895     gal\n",
      "20                           ISIP                 6795     Psi\n",
      "21                  Frac Gradient                 0.96  psi/ft\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "# input file\n",
    "filename = 'TCC Encana Oil  Gas - Horseshoe Hill 10H-1 Stg 3.pdf'\n",
    "# initiation of patterns\n",
    "patterns = my_patterns()  \n",
    "# get page from PyPDF2 as it follows the order\n",
    "page,page_number=find_correct_page_number(filename)\n",
    "# get text from pdfminer, as columns names are in the order\n",
    "text_string = convert_pdf_to_txt(filename,page_number)\n",
    "# get columns from text_string\n",
    "columns, stage = find_columns_names(text_string,patterns)\n",
    "# get value from page using columns from text_string\n",
    "# make it easier to extract\n",
    "page = page.split('Disclaimer')[0] \n",
    "# remove \\n from the page \n",
    "page_no_enter = remove_enter_from_string(page)\n",
    "# find the data contains value and unit\n",
    "data = find_data_names_from_string(page_no_enter,columns)\n",
    "# split data into value and unit\n",
    "data_value,data_unit = split_value_unit_from_string(data,patterns['seperate_value_unit'],columns)\n",
    "# create the dataframe\n",
    "DataFrame= pd.DataFrame({'1name':columns,'data_value':np.array(data_value),'unit':pd.Series(data_unit)}) \n",
    "print(DataFrame)\n",
    "\n",
    "\n",
    "#html_string = convert_pdf_to_html(filename,page_number)\n",
    "\n",
    "#patterns = dict()\n",
    "#patterns['stage']='.*Stage (\\d+) \\n'\n",
    "#patterns['text_name']='.*\\n([A-Z].*|\\d{1,3} [A-Z].*|\\d{1,3}/.*)'\n",
    "#patterns['text_name']='.*\\n([A-Z].*|\\d{1,3} [A-Z].*|\\d{1,3}/.*)'\n",
    "#patterns['html_name'] = '.*<br>.*12px\">(\\d.*\\n)'\n",
    "#text_split1 = text_string.split('Disclaimer')[0]\n",
    "\n",
    "#text_string_0 = text_string.index('Start Time') # find the first Start Time string\n",
    "#text_string_1 = text_string.index('Disclaimer')\n",
    "#text_split1 = text_string[text_string_0-1:text_string_1] # in order to contains \\n for the Start Time\n",
    "\n",
    "#html_string_0 = html_string.index('Start Time')\n",
    "#html_string_1 = html_string.index('Disclaimer')\n",
    "#html_split1 = html_string[html_string_0-1:html_string_1]\n",
    "\n",
    "#page_string_0 = page.index('Start Time')\n",
    "#page_string_1 = page.index('Disclaimer')\n",
    "#page_split1 = page[page_string_0-1:page_string_1]\n",
    "\n",
    "#print(string_0)\n",
    "# miner from text\n",
    "#columns_name_text = re.findall(patterns['text_name'],text_split1)\n",
    "#stage_number = re.findall(patterns['stage'],text_split1)\n",
    "\n",
    "\n",
    "#columns_name_text = [e.rstrip() for e in columns_name_text]\n",
    "\n",
    "#print(columns_name_text,len(columns_name_text))\n",
    "#print(page)\n",
    "\n",
    "#page_no_enter = page\n",
    "#print(page_no_enter)\n",
    "\n",
    "\n",
    "\n",
    "#a = page_no_enter.index('Water Frac G (5)')\n",
    "#print('index',a)\n",
    "#columns = ['Start Time','End Time','Max Treating Pressure']\n",
    "\n",
    "\n",
    "#print('data',data)\n",
    "#print('data',data1)\n",
    "#\n",
    "# find the value in the data \n",
    "\n",
    "#data_number \n",
    "#print(data_value)\n",
    "#print(text_split1)\n",
    "#print(page)\n",
    "\n",
    "#print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'columns_name_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0bae482443b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstring_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpopout\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpopout\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstring_new\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns_name_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_last_white_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'columns_name_text' is not defined"
     ]
    }
   ],
   "source": [
    "a = '123 3344 ddg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.96p'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_unit = '(\\d+|\\d*.\\d+|\\d*,\\d*)(?:[a-z\\s\\*])'\n",
    "match=re.search(pattern_unit,'.96psi/ft')\n",
    "match.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_start = list()\n",
    "index_end = list()\n",
    "for index,col in enumerate(columns):\n",
    "    #print('name',col)\n",
    "    if col in page_no_enter:\n",
    "        the_index_start = page_no_enter.index(col)\n",
    "        the_index_end = the_index_start + len(col)\n",
    "    elif remove_last_white_space(col,0) in page_no_enter:\n",
    "        \n",
    "        print('not in, try remove first')\n",
    "        \n",
    "        col_new = remove_last_white_space(col,0)\n",
    "        print(col_new)\n",
    "        the_index_start = page_no_enter.index(col_new)\n",
    "        the_index_end = the_index_start + len(col_new)\n",
    "    elif remove_last_white_space(col,-1) in page_no_enter:\n",
    "        print('not in, try remove last one')\n",
    "       \n",
    "        col_new = remove_last_white_space(col,-1)\n",
    "        #print(col_new)\n",
    "        the_index_start = page_no_enter.index(col_new)\n",
    "        the_index_end = the_index_start + len(col_new)\n",
    "    #print('index',the_index_start,the_index_end)\n",
    "    #print('col',page_no_enter[the_index_start:the_index_end])\n",
    "    #match = re.search(col,page_no_enter)\n",
    "    #print(match.start(),match.end())\n",
    "    \n",
    "    #index_end.append(match.start())\n",
    "    index_end.append(the_index_start)\n",
    "    if index == len(columns)-1: # end of column\n",
    "        index_end.append(len(page_no_enter))\n",
    "        index_end.pop(0)\n",
    "    #elif index > 0: \n",
    "    index_start.append(the_index_end)\n",
    "    #index_start.append(match.end()+1)\n",
    "#for index,e in enumerate(index_end):\n",
    "my_list = [(index_start[index],e) for index,e in enumerate(index_end)]\n",
    "#print(my_list)\n",
    "#name = \n",
    "data = [page_no_enter[e1:e2] for e1,e2 in my_list]\n",
    "pattern_unit = '(\\d+|\\d*.\\d+|\\d*,\\d*)(?:[a-z\\s\\*])'\n",
    "match=re.search(pattern_unit,'.96psi/ft')\n",
    "data_value = ['']*len(data)\n",
    "data_unit = ['']*len(data)\n",
    "for i,e in enumerate(data):\n",
    "    if 'Start Time' in columns_name_text[i] or 'End Time' in columns_name_text[i]:\n",
    "        data_value[i] = data[i].rstrip( )\n",
    "        data_unit[i] = 'sec'\n",
    "    else:\n",
    "        print('col',e)\n",
    "        match=re.search(pattern_unit,e)\n",
    "        data_value[i] = match.group(0).rstrip( )\n",
    "        if ',' in data_value[i]:\n",
    "            data_value[i]=data_value[i].replace(',','')\n",
    "        data_unit[i] = data[i][match.end():].rstrip( ) # remove the space at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "yes\n",
      "(2, 12)\n",
      "(39, 47)\n",
      "(74, 83)\n",
      "(95, 116)\n",
      "[(13, 39), (48, 74), (84, 95), (117, 713)]\n",
      "[u' 05\\n-Mar\\n-12 \\n20:37\\n:51\\n  ', u' 05\\n-Mar\\n-12 23:\\n05\\n:55\\n  ', u' 146\\n min\\n ', u' 9,495\\n psi\\n Avg Treating Pressure\\n 8,933\\n psi\\n Max Slurry \\nRate\\n 70.7\\n bpm Avg Slurry Rate\\n 69.9\\n bpm Slurry Volume\\n 415\\n,753\\n gal\\n Avg HHP\\n 15,304\\n hp Max Proppant Concentration\\n 4.84\\n lb/gal\\n BH Max Proppant Concentration\\n 4.85\\n lb/gal\\n Proppant Mass Pumped\\n 455,110\\n 100*lb\\n 100 Mesh\\n 40,060\\n lb 40/70 Premium White\\n 370,000\\n lb 30/50 Interprop\\n 45,050\\n lb Load to Recover\\n 395\\n,369\\n gal\\n Water Frac G (5)\\n 256,012\\n gal\\n Hybor G (27)\\n 139,357\\n gal\\n NaCl\\n 13,020\\n gal\\n Slurry Water Frac G (5)\\n 263,451\\n gal\\n Slurry Hybor G (27)\\n 152,302\\n gal\\n ISIP\\n 5,516\\n Psi\\n Frac Gradient\\n 0.\\n857\\n psi/ft\\n  ']\n"
     ]
    }
   ],
   "source": [
    "filename = 'TCC Encana Oil  Gas - Horseshoe Hill 10H-1 Stg 13.pdf'\n",
    "pdf = PyPDF2.PdfFileReader(open(filename, \"rb\")) \n",
    "page_number = 5\n",
    "find_page = False\n",
    "while page_number<10 and not find_page: \n",
    "    page = pdf.getPage(page_number).extractText()\n",
    "    if 'Job Summary' in page:\n",
    "        print('yes')\n",
    "        find_page = True\n",
    "    else:\n",
    "        print('no')\n",
    "        page_number += 1\n",
    "\n",
    "page = page.split('Job Summary')[1]\n",
    "page = page.split('Disclai')[0]\n",
    "#print(page.split('\\n'))\n",
    "test_page ='  Start Time123444End Time555555'\n",
    "columns = ['Start Time','End Time','Pump Time','Max Treating Pressure']\n",
    "#print(page)\n",
    "index_start = list()\n",
    "index_end = list()\n",
    "for index,col in enumerate(columns):\n",
    "    match = re.search(col,page)\n",
    "    print(match.start(),match.end())\n",
    "    \n",
    "    index_end.append(match.start())\n",
    "    if index == len(columns)-1:\n",
    "        index_end.append(len(page))\n",
    "        index_end.pop(0)\n",
    "    #elif index > 0: \n",
    "    index_start.append(match.end()+1)\n",
    "#for index,e in enumerate(index_end):\n",
    "my_list = [(index_start[index],e) for index,e in enumerate(index_end)]\n",
    "data = [page[e1:e2] for e1,e2 in my_list]\n",
    "#for index,e in enumerate(data):\n",
    "    #while '\\n' in e:\n",
    "    #data[index]=e.replace('\\n','')\n",
    "print(my_list)\n",
    "print(data[:])\n",
    "#print(index_start)\n",
    "#print(index_end)\n",
    "\n",
    "    #print(page[match.end()+1:match.end()+7])\n",
    "#data = page.split('\\n')\n",
    "#number_column = 7 \n",
    "#column_names = data[2:7]\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
