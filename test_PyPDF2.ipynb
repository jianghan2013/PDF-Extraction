{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter,HTMLConverter,XMLConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from cStringIO import StringIO\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_correct_page_number(filename):\n",
    "    pdf = PyPDF2.PdfFileReader(open(filename, \"rb\")) \n",
    "    page_number = 6 \n",
    "    page_index = page_number - 1\n",
    "  \n",
    "    find_page = False\n",
    "    while page_index<10 and not find_page: \n",
    "        page = pdf.getPage(page_index).extractText()\n",
    "        if 'Job Summary' in page:\n",
    "            print('correct page')\n",
    "            find_page = True\n",
    "        else:\n",
    "            print('no')\n",
    "            page_index += 1\n",
    "    page_number = page_index +1 \n",
    "    return page,page_number\n",
    "\n",
    "def convert_pdf_to_txt(path,page_number):\n",
    "    \n",
    "    page_index= page_number-1\n",
    "    rsrcmgr = PDFResourceManager() # task with interpreter and device\n",
    "    retstr = StringIO() # make it faster\n",
    "    codec = 'utf-8' # code format\n",
    "    laparams = LAParams() # help with document extraction\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams) # export into retstr\n",
    "    fp = file(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    pages= PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True)\n",
    "    for index,page in enumerate(pages):\n",
    "        if index == page_index:\n",
    "            interpreter.process_page(page)\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    string = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return string\n",
    "\n",
    "def convert_pdf_to_html(path,page_number):\n",
    "    #outfile = file(outpath,'w')\n",
    "    page_index= page_number-1\n",
    "    rsrcmgr = PDFResourceManager() # task with interpreter and device\n",
    "    retstr = StringIO() # make it faster\n",
    "    codec = 'utf-8' # code format\n",
    "    laparams = LAParams() # help with document extraction\n",
    "    device = HTMLConverter(rsrcmgr, retstr, codec=codec, laparams=laparams) # export into retstr\n",
    "    fp = file(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    pages= PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True)\n",
    "    for index,page in enumerate(pages):\n",
    "        if index == page_index:\n",
    "            interpreter.process_page(page)\n",
    "    \n",
    "    fp.close()\n",
    "    device.close()\n",
    "    string=retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return string\n",
    "\n",
    "def my_patterns():\n",
    "    patterns = dict()\n",
    "    patterns['stage']='.*Stage (\\d+) \\n'\n",
    "    patterns['text_name']='.*\\n([A-Z].*|\\d{1,3} [A-Z].*|\\d{1,3}/.*)'\n",
    "    patterns['seperate_value_unit'] = '(\\d+|\\d*.\\d+|\\d*,\\d*)(?:[a-z\\s\\*])'\n",
    "    return patterns\n",
    "\n",
    "def find_columns_names(text_string,patterns):\n",
    "    text_string_0 = text_string.index('Start Time') # find the first Start Time string\n",
    "    text_string_1 = text_string.index('Disclaimer')\n",
    "    text_split1 = text_string[text_string_0-1:text_string_1] # in order to contains \\n for the Start Time\n",
    "    columns_name_text = re.findall(patterns['text_name'],text_split1)\n",
    "    stage_number = re.findall(patterns['stage'],text_split1)\n",
    "    \n",
    "    # remove the white space \n",
    "    columns_name_text = [e.rstrip() for e in columns_name_text]\n",
    "    \n",
    "    #print(columns_name_text)\n",
    "    return columns_name_text,stage_number\n",
    "\n",
    "def remove_enter_from_string(string):\n",
    "    return string.replace('\\n','')\n",
    "\n",
    "def remove_last_white_space(string,index):\n",
    "    match = re.search('\\s',string)\n",
    "    index_space = [i for i in range(len(string)) if string.startswith(' ', i)]\n",
    "    popout = index_space.pop(index)\n",
    "    string_new = string[0:popout] + string[popout+1:]\n",
    "    return string_new\n",
    "\n",
    "\n",
    "def find_data_names_from_string(strings,columns):\n",
    "    index_start = list() # record the start of value\n",
    "    index_end = list() # recrod the end of the value\n",
    "    for index,col in enumerate(columns):\n",
    "        #print('name',col)\n",
    "        if col in strings:\n",
    "            the_index_start = strings.index(col)\n",
    "            the_index_end = the_index_start + len(col)\n",
    "        elif remove_last_white_space(col,0) in strings:        \n",
    "            print('not in, try remove first')\n",
    "            col_new = remove_last_white_space(col,0)\n",
    "            print(col_new)\n",
    "            the_index_start = strings.index(col_new)\n",
    "            the_index_end = the_index_start + len(col_new)\n",
    "        elif remove_last_white_space(col,-1) in strings:\n",
    "            print('not in, try remove last one')\n",
    "            col_new = remove_last_white_space(col,-1)\n",
    "            print(col_new)\n",
    "            the_index_start = strings.index(col_new)\n",
    "            the_index_end = the_index_start + len(col_new)\n",
    "        index_end.append(the_index_start)\n",
    "        if index == len(columns)-1: # end of column, the first element should be deleted\n",
    "            index_end.append(len(strings))\n",
    "            index_end.pop(0)\n",
    "        #elif index > 0: \n",
    "        index_start.append(the_index_end)\n",
    "        #index_start.append(match.end()+1)\n",
    "    #for index,e in enumerate(index_end):\n",
    "    index_range_list = [(index_start[index],e) for index,e in enumerate(index_end)]\n",
    "    #print(my_list)\n",
    "    #name = \n",
    "    data = [strings[e1:e2] for e1,e2 in index_range_list]\n",
    "    return data\n",
    "\n",
    "def split_value_unit_from_string(data,pattern,columns):\n",
    "    #pattern = \n",
    "    #match=re.search(pattern_unit,'.96psi/ft')\n",
    "    data_value = ['']*len(data) # record of values \n",
    "    data_unit = ['']*len(data) # record of units\n",
    "    for i,elem in enumerate(data):\n",
    "        if 'Start Time' in columns[i] or 'End Time' in columns[i]:\n",
    "            data_value[i] = elem.rstrip( )\n",
    "            data_unit[i] = 'sec'\n",
    "        else:\n",
    "            #print('col',elem)\n",
    "            match=re.search(pattern,elem)\n",
    "            data_value[i] = match.group(0).rstrip( )\n",
    "            if ',' in data_value[i]:\n",
    "                data_value[i]=data_value[i].replace(',','')\n",
    "                \n",
    "            data_unit[i] = elem[match.end():].rstrip( ) # remove the space at the end\n",
    "    return data_value,data_unit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "correct page\n",
      "                            1name           data_value    unit\n",
      "0                      Start Time   06-Mar-12 09:29:41     sec\n",
      "1                        End Time   06-Mar-12 13:31:29     sec\n",
      "2                       Pump Time               190.52     min\n",
      "3           Max Treating Pressure                11429     psi\n",
      "4           Avg Treating Pressure                10139     psi\n",
      "5                 Max Slurry Rate                 73.8     bpm\n",
      "6                 Avg Slurry Rate                 62.1     bpm\n",
      "7                   Slurry Volume               424446     gal\n",
      "8                         Avg HHP                15427      hp\n",
      "9      Max Proppant Concentration                 3.76  lb/gal\n",
      "10  BH Max Proppant Concentration                 3.76  lb/gal\n",
      "11           Proppant Mass Pumped              3416.91  100*lb\n",
      "12                       100 Mesh                40300      lb\n",
      "13             Premium White 4/70               274480      lb\n",
      "14                  Premium 30/50                47900      lb\n",
      "15                Load to Recover               470286     gal\n",
      "16               Water Frac G (5)               267911     gal\n",
      "17                   Hybor G (27)               150134     gal\n",
      "18                           NaCl                15182     gal\n",
      "19        Slurry Water Frac G (5)               270054     gal\n",
      "20            Slurry Hybor G (27)               154892     gal\n",
      "21                           ISIP                 8131     psi\n",
      "22                  Frac Gradient                 1.06  psi/ft\n"
     ]
    }
   ],
   "source": [
    "def main(filename):\n",
    "    # initiation of patterns\n",
    "    patterns = my_patterns()  \n",
    "    # get page from PyPDF2 as it follows the order\n",
    "    page,page_number=find_correct_page_number(filename)\n",
    "    # get text from pdfminer, as columns names are in the order\n",
    "    text_string = convert_pdf_to_txt(filename,page_number)\n",
    "    # get columns from text_string\n",
    "    columns, stage = find_columns_names(text_string,patterns)\n",
    "    # get value from page using columns from text_string\n",
    "    # make it easier to extract\n",
    "    page = page.split('Disclaimer')[0] \n",
    "    # remove \\n from the page \n",
    "    page_no_enter = remove_enter_from_string(page)\n",
    "    # find the data contains value and unit\n",
    "    data = find_data_names_from_string(page_no_enter,columns)\n",
    "    # split data into value and unit\n",
    "    data_value,data_unit = split_value_unit_from_string(data,patterns['seperate_value_unit'],columns)\n",
    "    # create the dataframe\n",
    "    DataFrame= pd.DataFrame({'1name':columns,'data_value':np.array(data_value),'unit':pd.Series(data_unit)})\n",
    "    \n",
    "    return DataFrame\n",
    "filename = 'TCC Encana Oil  Gas - Horseshoe Hill 10H-1 Stg 15.pdf'\n",
    "DataFrame = main(filename)\n",
    "print(DataFrame)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py27]",
   "language": "python",
   "name": "Python [py27]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
